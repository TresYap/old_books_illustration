{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'slugify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f9ca6bdadf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mslugify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mslugify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'slugify'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from joblib import Parallel, delayed\n",
    "from slugify import slugify\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.79 ms\n"
     ]
    }
   ],
   "source": [
    "headers = \"<replace with headers from logged-in session>\"\n",
    "\n",
    "headers = {head.split(':', maxsplit=1)[0]: head.split(':', maxsplit=1)[1].strip() for head in headers.split('\\n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.45 ms\n"
     ]
    }
   ],
   "source": [
    "# cookie = '''sck=OWJRH4PX45M2PZPUU6NST2B4RKRHPGXX3HSDAHY; SEID=s3; _ga=GA1.2.1000873538.1540110277; _gid=GA1.2.203592236.1540110277; jmi_scope=23|3315; uau=PQCKKEOCRBIHIC3A4YFMBYI33LU7GGTW4M5LPLZ7TIBOE7Y4PW3Q; _gat=1; jmi_period=2018-08'''\n",
    "# cookie = '''sck=LDEAYIHCEOYO5BPE4BWUDZ3ICUTODR7YJ22FWJY; SEID=s1; _ga=GA1.2.1688975569.1543245107; _gid=GA1.2.449345683.1543245107; _gat=1; uau=PUQAF7YDTPEPLGCEWFAMPD2RNYIEOCEUXA25WYDDEZ52ADHYH4HA; sa=\"\"; jmi_scope=23|3315; jmi_period=-1; jmi_mkt=\"\"'''\n",
    "cookie = \"<get from headers, looks like above.>\"  # THe cookie has some variables that can be used to control the scraping behavior and the kind of data that will be available, e.g., `jmi_period`\n",
    "cookie = {ck.split('=', maxsplit=1)[0]: ck.split('=', maxsplit=1)[1].strip() for ck in cookie.split('; ')}\n",
    "\n",
    "def build_cookie(cookie):\n",
    "    return '; '.join([f'{i}={j}' for i, j in cookie.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.08 ms\n"
     ]
    }
   ],
   "source": [
    "headers['Cookie'] = build_cookie(cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.79 ms\n"
     ]
    }
   ],
   "source": [
    "payload = '''primaryField: title\n",
    "sortTable: market\n",
    "scope: 23|3315\n",
    "secondaryField: \n",
    "period: -12\n",
    "periodCompare: 0\n",
    "sortOrder: \n",
    "offset: 0\n",
    "pageSize: 100\n",
    "secondaryOffset: 0\n",
    "category: \n",
    "company: \n",
    "location: \n",
    "title: \"Encoder\"^\"Data Encoder\"\n",
    "jobType: \n",
    "salary: \n",
    "comSize: \n",
    "comType: \n",
    "hotVacancies: \n",
    "comJobBoard: \n",
    "mkt.company: \n",
    "mkt.site: \n",
    "time: 181021171805'''\n",
    "\n",
    "payload = {pl.split(':', maxsplit=1)[0]: pl.split(':', maxsplit=1)[1].strip() for pl in payload.split('\\n')}\n",
    "\n",
    "payload = {\n",
    "    'category': '',\n",
    "    'comJobBoard': '',\n",
    "    'comSize': '',\n",
    "    'comType': '',\n",
    "    'company': '',\n",
    "    'hotVacancies': '',\n",
    "    'jobType': '',\n",
    "    'location': '',\n",
    "    'mkt.company': '',\n",
    "    'mkt.site': '',\n",
    "    'offset': '0',\n",
    "    'pageSize': '100',\n",
    "    'period': '-12',  # 12 months period\n",
    "    'periodCompare': '0',\n",
    "    'primaryField': 'title',\n",
    "    'salary': '',\n",
    "    'scope': '23|3315',\n",
    "    'secondaryField': '',\n",
    "    'secondaryOffset': '0',\n",
    "    'sortOrder': '',\n",
    "    'sortTable': 'market',\n",
    "    # 'time': '181021171805',\n",
    "    'title': '' # '\"Encoder\"^\"Data Encoder\"'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 43.6 ms\n"
     ]
    }
   ],
   "source": [
    "def get_jobs_table(payload):\n",
    "    url = 'https://www.jobmarketinsights.com/jmi-area/FullReport'\n",
    "\n",
    "    res = requests.get(url, headers=headers, params=payload)\n",
    "    b = BeautifulSoup(res.content, 'html5lib')\n",
    "\n",
    "    table = str(b.find('table'))\n",
    "    table = pd.read_html(table, skiprows=1)[0]\n",
    "    table.columns = ['job_title', 'ours', 'market', 'market_share']\n",
    "    table['market_count'] = table['market'].str.replace(',', '').str.replace(' .*', '').astype(int)\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def get_inspect_list(job_title, jmi_period):\n",
    "    inspect_payload = {'title': f'\"{job_title}\"'}\n",
    "    \n",
    "    head = headers.copy()\n",
    "    cook = cookie.copy()\n",
    "    cook['jmi_period'] = jmi_period\n",
    "    head['Cookie'] = build_cookie(cook)\n",
    "\n",
    "    url = 'https://www.jobmarketinsights.com/jmi-area/OfferInspectList'\n",
    "    \n",
    "    res = requests.get(url, headers=head, params=inspect_payload)\n",
    "    b = BeautifulSoup(res.content, 'html5lib')\n",
    "    \n",
    "    return re.findall('push\\(\"(.*)\"\\);', b.find('script').text)\n",
    "\n",
    "\n",
    "def get_item_inspect_list(item):\n",
    "    data = get_inspect_list(job_title=item['title'], jmi_period=item['period'])\n",
    "    item['ids'] = data\n",
    "    return item\n",
    "\n",
    "\n",
    "def get_single(job_ids):\n",
    "    single_payload = {'ids': job_ids}\n",
    "    url = 'https://www.jobmarketinsights.com/jmi-area/OfferInspectSingle'\n",
    "\n",
    "    res = requests.get(url, headers=headers, params=single_payload)\n",
    "    b = BeautifulSoup(res.content, 'html5lib')\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get jobs with at least 5 items in 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset: 10000\n",
      "Offset: 20000\n",
      "Offset: 30000\n",
      "Offset: 40000\n",
      "Offset: 50000\n",
      "Offset: 60000\n",
      "Offset: 70000\n",
      "Offset: 80000\n",
      "Offset: 90000\n",
      "Offset: 100000\n",
      "Offset: 110000\n",
      "Offset: 120000\n",
      "Offset: 130000\n",
      "Offset: 140000\n",
      "Offset: 150000\n",
      "Offset: 160000\n",
      "Offset: 170000\n",
      "Offset: 180000\n",
      "Offset: 190000\n",
      "Offset: 200000\n",
      "Offset: 210000\n",
      "Offset: 220000\n",
      "Offset: 230000\n",
      "Offset: 240000\n",
      "Offset: 250000\n",
      "Offset: 260000\n",
      "Offset: 270000\n",
      "Offset: 280000\n",
      "Offset: 290000\n",
      "Offset: 300000\n",
      "Offset: 310000\n",
      "Offset: 330000\n",
      "Offset: 340000\n",
      "Offset: 350000\n",
      "Offset: 360000\n",
      "Offset: 370000\n",
      "Offset: 380000\n",
      "Offset: 390000\n",
      "Offset: 400000\n",
      "Offset: 410000\n",
      "time: 3h 27min 59s\n"
     ]
    }
   ],
   "source": [
    "full_table = pd.DataFrame()\n",
    "inspect_list_info = []\n",
    "MIN_MARKET_COUNT = 1\n",
    "MAX_OFFSET = 500000\n",
    "\n",
    "# popular_titles = []\n",
    "last_offset = 0\n",
    "page_size = 100\n",
    "payload['pageSize'] = page_size\n",
    "payload['title'] = ''\n",
    "\n",
    "payload['period'] = '-12'\n",
    "\n",
    "for offset in range(0, MAX_OFFSET + page_size, page_size):\n",
    "    if offset and offset % 10000 == 0:\n",
    "        print(f'Offset: {offset}')\n",
    "        full_table.to_csv(f'jobs-full-report_period-{payload[\"period\"]}_date-{datetime.now().date()}.csv', index=False)\n",
    "\n",
    "    payload['offset'] = offset\n",
    "\n",
    "    try:\n",
    "        table = get_jobs_table(payload)\n",
    "    except AttributeError:\n",
    "        break\n",
    "\n",
    "    popular = table[table.market_count >= MIN_MARKET_COUNT]\n",
    "    # popular_titles.extend(popular.job_title.map(lambda x: f'\"{x}\"').tolist())\n",
    "\n",
    "    if full_table.empty:\n",
    "        full_table = table\n",
    "    else:\n",
    "        full_table = pd.concat([full_table, table])\n",
    "\n",
    "    if popular.empty:\n",
    "        break\n",
    "        \n",
    "    last_offset = offset\n",
    "\n",
    "    time.sleep(min(np.random.exponential(1), 1.5))\n",
    "    \n",
    "full_table.to_csv(f'jobs-full-report_period-{payload[\"period\"]}_date-{datetime.now().date()}.csv', index=False)\n",
    "\n",
    "for row, item in full_table[full_table.market_count <= 500].iterrows():\n",
    "    inspect_list_info.append({\n",
    "        'title': item['job_title'],\n",
    "        'count': item['market_count'],\n",
    "        'comSize': '',\n",
    "        'period': payload['period'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413000, 412900)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.91 ms\n"
     ]
    }
   ],
   "source": [
    "offset, last_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 397500)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.37 ms\n"
     ]
    }
   ],
   "source": [
    "offset, last_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152400, 152400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.4 ms\n"
     ]
    }
   ],
   "source": [
    "offset, last_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset: 160000\n",
      "Offset: 170000\n",
      "Offset: 180000\n",
      "Offset: 190000\n",
      "Offset: 200000\n",
      "Offset: 210000\n",
      "Offset: 220000\n",
      "Offset: 230000\n",
      "Offset: 240000\n",
      "Offset: 250000\n",
      "Offset: 260000\n",
      "Offset: 270000\n",
      "Offset: 280000\n",
      "Offset: 290000\n",
      "Offset: 300000\n",
      "Offset: 310000\n",
      "Offset: 320000\n",
      "Offset: 330000\n",
      "Offset: 340000\n",
      "Offset: 350000\n",
      "Offset: 360000\n",
      "Offset: 370000\n",
      "time: 1h 26min 54s\n"
     ]
    }
   ],
   "source": [
    "# for offset in range(last_offset + page_size, MAX_OFFSET + page_size, page_size):\n",
    "#     if offset and offset % 10000 == 0:\n",
    "#         print(f'Offset: {offset}')\n",
    "#         full_table.to_csv(f'jobs-full-report_period-{payload[\"period\"]}_date-{datetime.now().date()}.csv', index=False)\n",
    "\n",
    "#     payload['offset'] = offset\n",
    "\n",
    "#     try:\n",
    "#         table = get_jobs_table(payload)\n",
    "#     except AttributeError:\n",
    "#         break\n",
    "\n",
    "#     popular = table[table.market_count >= MIN_MARKET_COUNT]\n",
    "#     # popular_titles.extend(popular.job_title.map(lambda x: f'\"{x}\"').tolist())\n",
    "\n",
    "#     if full_table.empty:\n",
    "#         full_table = table\n",
    "#     else:\n",
    "#         full_table = pd.concat([full_table, table])\n",
    "\n",
    "#     if popular.empty:\n",
    "#         break\n",
    "        \n",
    "#     last_offset = offset\n",
    "\n",
    "#     time.sleep(min(np.random.exponential(1), 1.5))\n",
    "    \n",
    "# full_table.to_csv(f'jobs-full-report_period-{payload[\"period\"]}_date-{datetime.now().date()}.csv', index=False)\n",
    "\n",
    "# for row, item in full_table[full_table.market_count <= 500].iterrows():\n",
    "#     inspect_list_info.append({\n",
    "#         'title': item['job_title'],\n",
    "#         'count': item['market_count'],\n",
    "#         'comSize': '',\n",
    "#         'period': payload['period'],\n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process for all titles greater than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412825\n",
      "time: 9.82 ms\n"
     ]
    }
   ],
   "source": [
    "print(len(inspect_list_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398294\n",
      "time: 973 µs\n"
     ]
    }
   ],
   "source": [
    "print(len(inspect_list_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379152\n",
      "time: 510 µs\n"
     ]
    }
   ],
   "source": [
    "print(len(inspect_list_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.38 ms\n"
     ]
    }
   ],
   "source": [
    "# inspect_list_info = []\n",
    "\n",
    "# for row, item in full_table[full_table.market_count <= 500].iterrows():\n",
    "#     inspect_list_info.append({\n",
    "#         'title': item['job_title'],\n",
    "#         'count': item['market_count'],\n",
    "#         'comSize': '',\n",
    "#         'period': '-12',\n",
    "#     })\n",
    "    \n",
    "# len(inspect_list_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.88 ms\n"
     ]
    }
   ],
   "source": [
    "popular_titles = full_table[full_table.market_count > 500].job_title.map(lambda x: f'\"{x}\"').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.5 ms\n"
     ]
    }
   ],
   "source": [
    "len(popular_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.08 ms\n"
     ]
    }
   ],
   "source": [
    "len(popular_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For jobs with more than 500 posts in 12 months, increase granularity by using Quarterly jmi_period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.9 ms\n"
     ]
    }
   ],
   "source": [
    "quarterly_periods = {\n",
    "    # 'Q4.2017': ['2017-10', '2017-11', '2017-12'],\n",
    "    'Q1.2018': ['2018-01', '2018-02', '2018-03'],\n",
    "    'Q2.2018': ['2018-04', '2018-05', '2018-06'],\n",
    "    'Q3.2018': ['2018-07', '2018-08', '2018-09'],\n",
    "    'Q4.2018': ['2018-10', '2018-11', '2018-12'],\n",
    "    'Q1.2019': ['2019-01', '2019-02', '2019-03'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.2018: 0\n",
      "Q1.2018: 100\n",
      "Q1.2018: 200\n",
      "2018-01: 0\n",
      "2018-01: 100\n",
      "2018-02: 0\n",
      "2018-02: 100\n",
      "2018-03: 0\n",
      "2018-03: 100\n",
      "Q2.2018: 0\n",
      "Q2.2018: 100\n",
      "Q2.2018: 200\n",
      "2018-04: 0\n",
      "2018-04: 100\n",
      "2018-05: 0\n",
      "2018-05: 100\n",
      "2018-06: 0\n",
      "2018-06: 100\n",
      "Q3.2018: 0\n",
      "Q3.2018: 100\n",
      "Q3.2018: 200\n",
      "2018-07: 0\n",
      "2018-07: 100\n",
      "2018-08: 0\n",
      "2018-08: 100\n",
      "2018-09: 0\n",
      "2018-09: 100\n",
      "Q4.2018: 0\n",
      "Q4.2018: 100\n",
      "Q4.2018: 200\n",
      "2018-10: 0\n",
      "2018-10: 100\n",
      "2018-11: 0\n",
      "2018-11: 100\n",
      "2018-12: 0\n",
      "2018-12: 100\n",
      "Q1.2019: 0\n",
      "Q1.2019: 100\n",
      "Q1.2019: 200\n",
      "2019-01: 0\n",
      "2019-01: 100\n",
      "2019-02: 0\n",
      "2019-02: 100\n",
      "2019-03: 0\n",
      "2019-03: 100\n",
      "413853\n",
      "time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "for period in quarterly_periods:\n",
    "    payload['title'] = '^'.join(popular_titles)\n",
    "    payload['offset'] = 0\n",
    "    payload['period'] = period\n",
    "\n",
    "    quarterly_full_table = pd.DataFrame()\n",
    "    \n",
    "    for offset in range(0, len(popular_titles) + 100, 100):\n",
    "        payload['offset'] = offset\n",
    "\n",
    "        print(f'{period}: {offset}')\n",
    "\n",
    "        payload['offset'] = offset\n",
    "\n",
    "        try:\n",
    "            table = get_jobs_table(payload)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        if quarterly_full_table.empty:\n",
    "            quarterly_full_table = table\n",
    "        else:\n",
    "            quarterly_full_table = pd.concat([quarterly_full_table, table])\n",
    "\n",
    "        time.sleep(min(np.random.exponential(1), 1.5))\n",
    "        \n",
    "    quarterly_popular_title = quarterly_full_table[quarterly_full_table.market_count > 500].job_title.map(lambda x: f'\"{x}\"').tolist()\n",
    "    for row, item in quarterly_full_table[quarterly_full_table.market_count <= 500].iterrows():\n",
    "        inspect_list_info.insert(0, {\n",
    "            'title': item['job_title'],\n",
    "            'count': item['market_count'],\n",
    "            'comSize': '',\n",
    "            'period': period,\n",
    "        })\n",
    "\n",
    "    for monthly_period in quarterly_periods[period]:\n",
    "        payload['title'] = '^'.join(quarterly_popular_title)\n",
    "        payload['offset'] = 0\n",
    "        payload['period'] = monthly_period\n",
    "\n",
    "        monthly_full_table = pd.DataFrame()\n",
    "\n",
    "        for offset in range(0, len(quarterly_popular_title) + 100, 100):\n",
    "            payload['offset'] = offset\n",
    "\n",
    "            print(f'{monthly_period}: {offset}')\n",
    "\n",
    "            payload['offset'] = offset\n",
    "\n",
    "            try:\n",
    "                table = get_jobs_table(payload)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            if monthly_full_table.empty:\n",
    "                monthly_full_table = table\n",
    "            else:\n",
    "                monthly_full_table = pd.concat([monthly_full_table, table])\n",
    "\n",
    "            time.sleep(min(np.random.exponential(1), 1.5))\n",
    "\n",
    "        for row, item in monthly_full_table.iterrows():\n",
    "            inspect_list_info.insert(0, {\n",
    "                'title': item['job_title'],\n",
    "                'count': item['market_count'],\n",
    "                'comSize': '',\n",
    "                'period': monthly_period,\n",
    "            })\n",
    "        \n",
    "        # NOTE: We're missing out on jobs with more than 500 postings in a month\n",
    "        \n",
    "print(len(inspect_list_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>ours</th>\n",
       "      <th>market</th>\n",
       "      <th>market_share</th>\n",
       "      <th>market_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accounting Staff</td>\n",
       "      <td>14 (18.2%)</td>\n",
       "      <td>1,193 (28.5%)</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounting Assistant</td>\n",
       "      <td>33 (42.9%)</td>\n",
       "      <td>971 (23.2%)</td>\n",
       "      <td>3.4%</td>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Office Staff</td>\n",
       "      <td>8 (10.4%)</td>\n",
       "      <td>902 (21.6%)</td>\n",
       "      <td>0.9%</td>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>17 (22.1%)</td>\n",
       "      <td>592 (14.1%)</td>\n",
       "      <td>2.9%</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Customer Service Representative</td>\n",
       "      <td>5 (6.5%)</td>\n",
       "      <td>526 (12.6%)</td>\n",
       "      <td>1.0%</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         job_title        ours         market market_share  \\\n",
       "0                 Accounting Staff  14 (18.2%)  1,193 (28.5%)         1.2%   \n",
       "1             Accounting Assistant  33 (42.9%)    971 (23.2%)         3.4%   \n",
       "2                     Office Staff   8 (10.4%)    902 (21.6%)         0.9%   \n",
       "3                  Sales Executive  17 (22.1%)    592 (14.1%)         2.9%   \n",
       "4  Customer Service Representative    5 (6.5%)    526 (12.6%)         1.0%   \n",
       "\n",
       "   market_count  \n",
       "0          1193  \n",
       "1           971  \n",
       "2           902  \n",
       "3           592  \n",
       "4           526  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.33 ms\n"
     ]
    }
   ],
   "source": [
    "monthly_full_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract job ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.14 ms\n"
     ]
    }
   ],
   "source": [
    "parallel_context = Parallel(n_jobs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "all_job_ids = set()\n",
    "# inspect_list_info = parallel_context(delayed(get_item_inspect_list)(item) for item in inspect_list_info)\n",
    "temp_inspect_list_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comSize': '',\n",
       " 'count': 8,\n",
       " 'ids': ['1326934661',\n",
       "  '1400623221^1399935727',\n",
       "  '1448814550',\n",
       "  '1482660929',\n",
       "  '1497661532',\n",
       "  '1656965960^1654391924^1546363231',\n",
       "  '1559381999',\n",
       "  '1565777151^1565023390^1564661180^1564135397'],\n",
       " 'period': '-12',\n",
       " 'title': 'Graphic Artist / Web Developer'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.8 ms\n"
     ]
    }
   ],
   "source": [
    "temp_inspect_list_info[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "scraped_titles = set([i['title'] for i in temp_inspect_list_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 10000...\n",
      "Elapsed time: 0.00020551681518554688 seconds...\n",
      "Item 20000...\n",
      "Elapsed time: 6.914138793945312e-05 seconds...\n",
      "Item 30000...\n",
      "Elapsed time: 4.696846008300781e-05 seconds...\n",
      "Item 40000...\n",
      "Elapsed time: 8.916854858398438e-05 seconds...\n",
      "Item 50000...\n",
      "Elapsed time: 4.601478576660156e-05 seconds...\n",
      "Item 60000...\n",
      "Elapsed time: 5.9604644775390625e-05 seconds...\n",
      "Item 70000...\n",
      "Elapsed time: 4.220008850097656e-05 seconds...\n",
      "Item 80000...\n",
      "Elapsed time: 3.814697265625e-05 seconds...\n",
      "Item 90000...\n",
      "Elapsed time: 4.1961669921875e-05 seconds...\n",
      "Item 100000...\n",
      "Elapsed time: 4.1484832763671875e-05 seconds...\n",
      "Item 110000...\n",
      "Elapsed time: 4.0531158447265625e-05 seconds...\n",
      "Item 120000...\n",
      "Elapsed time: 4.172325134277344e-05 seconds...\n",
      "Item 130000...\n",
      "Elapsed time: 76.80289435386658 seconds...\n",
      "Item 140000...\n",
      "Elapsed time: 110.13988065719604 seconds...\n",
      "Item 150000...\n",
      "Elapsed time: 215.78656578063965 seconds...\n",
      "Item 160000...\n",
      "Elapsed time: 81.26325583457947 seconds...\n",
      "Item 170000...\n",
      "Elapsed time: 215.07726979255676 seconds...\n",
      "Item 180000...\n",
      "Elapsed time: 82.21202802658081 seconds...\n",
      "Item 190000...\n",
      "Elapsed time: 111.50398325920105 seconds...\n",
      "Item 200000...\n",
      "Elapsed time: 108.94574165344238 seconds...\n",
      "Item 210000...\n",
      "Elapsed time: 110.56799054145813 seconds...\n",
      "Item 220000...\n",
      "Elapsed time: 112.84965991973877 seconds...\n",
      "Item 230000...\n",
      "Elapsed time: 110.80199718475342 seconds...\n",
      "Item 240000...\n",
      "Elapsed time: 109.1518714427948 seconds...\n",
      "Item 250000...\n",
      "Elapsed time: 212.9820854663849 seconds...\n",
      "Item 260000...\n",
      "Elapsed time: 78.21650409698486 seconds...\n",
      "Item 270000...\n",
      "Elapsed time: 111.45426487922668 seconds...\n",
      "Item 280000...\n",
      "Elapsed time: 111.17616415023804 seconds...\n",
      "Item 290000...\n",
      "Elapsed time: 108.75667357444763 seconds...\n",
      "Item 300000...\n",
      "Elapsed time: 80.08262777328491 seconds...\n",
      "Item 320000...\n",
      "Elapsed time: 109.16130661964417 seconds...\n",
      "Item 330000...\n",
      "Elapsed time: 111.18257093429565 seconds...\n",
      "Item 340000...\n",
      "Elapsed time: 109.15384435653687 seconds...\n",
      "Item 350000...\n",
      "Elapsed time: 110.59726142883301 seconds...\n",
      "Item 360000...\n",
      "Elapsed time: 109.34439897537231 seconds...\n",
      "Item 370000...\n",
      "Elapsed time: 214.02240109443665 seconds...\n",
      "Item 380000...\n",
      "Elapsed time: 76.03534269332886 seconds...\n",
      "Item 390000...\n",
      "Elapsed time: 89.27568936347961 seconds...\n",
      "Item 400000...\n",
      "Elapsed time: 110.71940565109253 seconds...\n",
      "Item 410000...\n",
      "time: 59min 19s\n"
     ]
    }
   ],
   "source": [
    "with Parallel(n_jobs=200) as parallel_context:\n",
    "    tlist_info = []\n",
    "    for ix, item in enumerate(inspect_list_info):\n",
    "        if ix and ix % 10000 == 0:\n",
    "            print(f'Item {ix}...')\n",
    "            start = time.time()\n",
    "            tlist_info = parallel_context(delayed(get_item_inspect_list)(item) for item in tlist_info)\n",
    "            temp_inspect_list_info.extend(tlist_info)\n",
    "            \n",
    "            for item in tlist_info:\n",
    "                scraped_titles.add(item['title'])\n",
    "\n",
    "            tlist_info = []\n",
    "            print(f'Elapsed time: {time.time() - start} seconds...')\n",
    "            \n",
    "        else:\n",
    "            if item['title'] not in scraped_titles:\n",
    "                tlist_info.append(item)\n",
    "\n",
    "    if tlist_info:\n",
    "        print(f'Item {ix}...')\n",
    "        tlist_info = parallel_context(delayed(get_item_inspect_list)(item) for item in tlist_info)\n",
    "        temp_inspect_list_info.extend(tlist_info)\n",
    "\n",
    "inspect_list_info = temp_inspect_list_info\n",
    "\n",
    "for item in inspect_list_info:\n",
    "    all_job_ids.update(item['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_job_ids = set()\n",
    "\n",
    "# for ix, item in enumerate(inspect_list_info):\n",
    "#     if ix and ix % 100 == 0:\n",
    "#         print(f\"{ix}: {len(all_job_ids)}\")\n",
    "\n",
    "#     data = get_inspect_list(job_title=item['title'], jmi_period=item['period'])\n",
    "#     all_job_ids.update(data)\n",
    "\n",
    "#     item['ids'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1069587"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.5 ms\n"
     ]
    }
   ],
   "source": [
    "len(all_job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024513"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.61 ms\n"
     ]
    }
   ],
   "source": [
    "len(all_job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "968433"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.6 ms\n"
     ]
    }
   ],
   "source": [
    "len(all_job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506573"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.92 ms\n"
     ]
    }
   ],
   "source": [
    "len(all_job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "with open(f'all_job_ids-{datetime.now().date()}.pkl', 'wb') as fl:\n",
    "    pickle.dump(list(all_job_ids), fl)\n",
    "\n",
    "with open(f'inspect_list_info-{datetime.now().date()}.pkl', 'wb') as fl:\n",
    "    pickle.dump(inspect_list_info, fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.9 ms\n"
     ]
    }
   ],
   "source": [
    "def get_single(job_ids):\n",
    "    single_payload = {'ids': job_ids}\n",
    "    url = 'https://www.jobmarketinsights.com/jmi-area/OfferInspectSingle'\n",
    "\n",
    "    res = requests.get(url, headers=headers, params=single_payload)\n",
    "\n",
    "    return res.content\n",
    "\n",
    "\n",
    "def scrape_items_to_file(item):\n",
    "    downloaded = {}\n",
    "\n",
    "    for ids, hash_ids in zip(item['ids'], item['hash_ids']):\n",
    "        title = item[\"title\"][:100]\n",
    "        fname = './jmi-jobs-data/' + slugify(f'{title} {hash_ids}') + '.html'\n",
    "        if os.path.isfile(fname):\n",
    "            downloaded[fname.split('/')[-1]] = False\n",
    "            continue\n",
    "\n",
    "        content = get_single(ids)\n",
    "        with open(fname, 'wb') as fl:\n",
    "            fl.write(content)\n",
    "\n",
    "        downloaded[fname.split('/')[-1]] = True\n",
    "\n",
    "    return downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.88 ms\n"
     ]
    }
   ],
   "source": [
    "cook = cookie.copy()\n",
    "cook['jmi_period'] = '-12'\n",
    "headers['Cookie'] = build_cookie(cook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df = pd.DataFrame(inspect_list_info)\n",
    "inspect_list_info_df['hash_ids'] = inspect_list_info_df.ids.map(lambda x: [hashlib.md5(i.encode('utf-8')).hexdigest() for i in x])\n",
    "inspect_list_info_df['posting_count'] = inspect_list_info_df.ids.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df.to_csv(f'inspect_list_info_df-{datetime.now().date()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.12 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avsolatorio/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df = pd.read_csv(f'inspect_list_info_df-{datetime.now().date()}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1121531"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.34 ms\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df.posting_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1121531"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.29 ms\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df.posting_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1062285"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df.posting_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993942"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.76 ms\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df.posting_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532081"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.14 ms\n"
     ]
    }
   ],
   "source": [
    "inspect_list_info_df.posting_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.64 ms\n"
     ]
    }
   ],
   "source": [
    "# time: 1h 22min 7s\n",
    "scrape_statuses = []\n",
    "with Parallel(n_jobs=200) as parallel_context:\n",
    "    temp_items = []\n",
    "    for row_ix, item in inspect_list_info_df.sort_values('posting_count', ascending=False).iterrows():\n",
    "        if row_ix and row_ix % 10000 == 0:\n",
    "            print(f'Item {row_ix}...')\n",
    "            start = time.time()\n",
    "            scrape_statuses.extend(parallel_context(delayed(scrape_items_to_file)(item) for item in temp_items))\n",
    "            temp_items = []\n",
    "            \n",
    "            print(f'Elapsed time: {time.time() - start} seconds...')\n",
    "        else:\n",
    "            temp_items.append(item)\n",
    "            \n",
    "    if temp_items:\n",
    "        print(f'Item {row_ix}...')\n",
    "        start = time.time()\n",
    "        scrape_statuses.extend(parallel_context(delayed(scrape_items_to_file)(item) for item in temp_items))\n",
    "        temp_items = []\n",
    "\n",
    "        print(f'Elapsed time: {time.time() - start} seconds...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.86 ms\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.07 ms\n"
     ]
    }
   ],
   "source": [
    "# # time: 1h 22min 7s\n",
    "# if __name__ == '__main__':\n",
    "#     parallel_context = mp.Pool(processes=200)              # start 4 worker processes\n",
    "\n",
    "#     scrape_statuses = []\n",
    "#     temp_items = []\n",
    "\n",
    "#     for row_ix, item in inspect_list_info_df.sort_values('posting_count', ascending=False).iterrows():\n",
    "#         if row_ix and row_ix % 1000 == 0:\n",
    "#             print(f'Item {row_ix}...')\n",
    "#             start = time.time()\n",
    "\n",
    "#             multiple_results = list(parallel_context.imap_unordered(scrape_items_to_file, temp_items))\n",
    "#             scrape_statuses.extend(multiple_results)\n",
    "\n",
    "#             temp_items = []\n",
    "            \n",
    "#             print(f'Elapsed time: {time.time() - start} seconds...')\n",
    "#         else:\n",
    "#             temp_items.append(item)\n",
    "            \n",
    "#     if temp_items:\n",
    "#         print(f'Item {row_ix}...')\n",
    "#         start = time.time()\n",
    "            \n",
    "#         multiple_results = list(parallel_context.imap_unordered(scrape_items_to_file, temp_items))\n",
    "#         scrape_statuses.extend(multiple_results)\n",
    "\n",
    "#         temp_items = []\n",
    "\n",
    "#         print(f'Elapsed time: {time.time() - start} seconds...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398294"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.4 ms\n"
     ]
    }
   ],
   "source": [
    "len(scrape_statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_statuses = scrape_statuses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024044"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.15 ms\n"
     ]
    }
   ],
   "source": [
    "full_table.market_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511476"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.25 ms\n"
     ]
    }
   ],
   "source": [
    "full_table.market_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
